{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG solution using Azure AI Search\n",
    "\n",
    "Steps in this notebook:\n",
    "1. Create an index \n",
    "2. Create a data source\n",
    "3. Create a skillset\n",
    "4. Create an indexer \n",
    "5. Send a query to the search engine to check results\n",
    "6. Send query results to a language model to generate response\n",
    "\n",
    "Note: Steps 1-4: Done during initial setup of Search Index only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure configurations\n",
    "\n",
    "You always need to run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_openai_embeddings_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "azure_openai_api_version = \"2024-12-01-preview\"\n",
    "azure_search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_search_service_admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "azure_search_service_index_name = \"az-search-index-001\"\n",
    "azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "azure_ai_services_key = os.getenv(\"AZURE_AI_MULTISERVICE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az-search-index-001 created\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    ScoringProfile,\n",
    "    TagScoringFunction,\n",
    "    TagScoringParameters\n",
    ")\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "\n",
    "# Search index name  \n",
    "index_name = azure_search_service_index_name\n",
    "\n",
    "# Create a Search Index Client\n",
    "index_client = SearchIndexClient(endpoint=azure_search_service_endpoint, credential=credential)\n",
    "\n",
    "# Define the fields collection\n",
    "fields = [\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"locations\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
    "    SearchField(name=\"text_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1024, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnsw\"),\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "        )\n",
    "    ],  \n",
    "    vectorizers=[   # a vectorizer is software that performs vectorization\n",
    "        AzureOpenAIVectorizer(  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            parameters=AzureOpenAIVectorizerParameters(  \n",
    "                resource_url=azure_openai_endpoint,  \n",
    "                deployment_name=azure_openai_embeddings_deployment,\n",
    "                model_name=azure_openai_embeddings_deployment\n",
    "            ),\n",
    "        ),  \n",
    "    ], \n",
    ")  \n",
    "\n",
    "# New semantic configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"locations\")],\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# New scoring profile\n",
    "scoring_profiles = [  \n",
    "    ScoringProfile(  \n",
    "        name=\"my-scoring-profile\",\n",
    "        functions=[\n",
    "            TagScoringFunction(  \n",
    "                field_name=\"locations\",  \n",
    "                boost=5.0,  \n",
    "                parameters=TagScoringParameters(  \n",
    "                    tags_parameter=\"tags\",  \n",
    "                ),  \n",
    "            ) \n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, \n",
    "                    fields=fields, \n",
    "                    vector_search=vector_search,\n",
    "                    semantic_search=semantic_search,\n",
    "                    scoring_profiles=scoring_profiles)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Azure Storage access\n",
    "Verify access to storage and print out the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blobs in the container:\n",
      "page-11.pdf\n",
      "page-13.pdf\n",
      "page-15.pdf\n",
      "page-17.pdf\n",
      "page-19.pdf\n",
      "page-21.pdf\n",
      "page-23.pdf\n",
      "page-25.pdf\n",
      "page-27.pdf\n",
      "page-31.pdf\n",
      "page-33.pdf\n",
      "page-35.pdf\n",
      "page-39.pdf\n",
      "page-41.pdf\n",
      "page-43.pdf\n",
      "page-45.pdf\n",
      "page-49.pdf\n",
      "page-51.pdf\n",
      "page-55.pdf\n",
      "page-57.pdf\n",
      "page-59.pdf\n",
      "page-61.pdf\n",
      "page-63.pdf\n",
      "page-65.pdf\n",
      "page-67.pdf\n",
      "page-7.pdf\n",
      "page-8.pdf\n",
      "page-9.pdf\n",
      "Access to the blob storage was granted.\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Initialize the BlobServiceClient with the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(azure_storage_connection_string)\n",
    "\n",
    "# Get the container client\n",
    "container_client = blob_service_client.get_container_client(\"nasa-books-pdf\")\n",
    "\n",
    "# List blobs in the container\n",
    "try:\n",
    "    blobs_list = container_client.list_blobs()\n",
    "    print(\"Blobs in the container:\")\n",
    "    for blob in blobs_list:\n",
    "        print(blob.name)\n",
    "    print(\"Access to the blob storage was granted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to access the blob storage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'ai-search-ds' created or updated\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(endpoint=azure_search_service_endpoint, credential=credential)\n",
    "container = SearchIndexerDataContainer(name=\"nasa-books-pdf\")\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=\"ai-search-ds\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=azure_storage_connection_string,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-search-ss created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    EntityRecognitionSkill,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset,\n",
    "    CognitiveServicesAccountKey\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = \"ai-search-ss\"\n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=2000,  \n",
    "    page_overlap_length=500,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_url=azure_openai_endpoint,  \n",
    "    deployment_name=azure_openai_embeddings_deployment,  \n",
    "    model_name=azure_openai_embeddings_deployment,\n",
    "    dimensions=1024,\n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"text_vector\")  \n",
    "    ],  \n",
    ")\n",
    "\n",
    "entity_skill = EntityRecognitionSkill(\n",
    "    description=\"Skill to recognize entities in text\",\n",
    "    context=\"/document/pages/*\",\n",
    "    categories=[\"Location\"],\n",
    "    default_language_code=\"en\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"locations\", target_name=\"locations\")\n",
    "    ]\n",
    ")\n",
    "  \n",
    "index_projections = SearchIndexerIndexProjection(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=azure_search_service_index_name,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
    "                InputFieldMappingEntry(name=\"text_vector\", source=\"/document/pages/*/text_vector\"),\n",
    "                InputFieldMappingEntry(name=\"locations\", source=\"/document/pages/*/locations\"),  \n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ") \n",
    "\n",
    "cognitive_services_account = CognitiveServicesAccountKey(key=azure_ai_services_key)\n",
    "\n",
    "skills = [split_skill, embedding_skill, entity_skill]\n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents, generate embeddings, and extract location entities\",  \n",
    "    skills=skills,  \n",
    "    index_projection=index_projections,\n",
    "    cognitive_services_account=cognitive_services_account\n",
    ")\n",
    "  \n",
    "client = SearchIndexerClient(endpoint=azure_search_service_endpoint, credential=credential)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"{skillset.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ai-search-indexer is created and running. Give the indexer a few minutes before running a query.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    IndexingSchedule\n",
    ")\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = \"ai-search-indexer\" \n",
    "\n",
    "# Schedule to run every 24 hours\n",
    "schedule = IndexingSchedule(interval=\"P1D\")\n",
    "\n",
    "indexer_parameters = None\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents, generate embeddings, and extract entities\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,\n",
    "    parameters=indexer_parameters,\n",
    "    schedule=schedule\n",
    ")  \n",
    "\n",
    "# Create and run the indexer  \n",
    "indexer_client = SearchIndexerClient(endpoint=azure_search_service_endpoint, credential=credential)  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "\n",
    "print(f' {indexer_name} is created and running. Give the indexer a few minutes before running a query.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a query to the search engine to check results\n",
    "\n",
    "Executed every time a user makes a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.03253968432545662 \n",
      "\n",
      "Title: page-35.pdf \n",
      "\n",
      "Chunk: A\n",
      "T\n",
      "\n",
      "M\n",
      "O\n",
      "\n",
      "S\n",
      "P\n",
      "\n",
      "H\n",
      "E\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "28\n",
      "\n",
      "Four Mountains Stand Out\n",
      "Pacific Ocean\n",
      "\n",
      "They are called the Islands of the Four Mountains. Part of the Aleutian Island chain, these peaks are actually the upper slopes of \n",
      "\n",
      "volcanoes rising from the seafloor: Carlisle, Cleveland, Herbert, and Tana. Standing in one of the most remote reaches of the world, \n",
      "\n",
      "these volcanoes have scarcely been studied. Satellite sensing makes that easier, as this Landsat 8 image from June 2013 shows.\n",
      "\n",
      "Herbert Island (right) is dominated by a symmetrical stratovolcano that stands in its center. The remote island has scarcely been \n",
      "\n",
      "studied, and there are no records of eruptions there. The 2-kilometer-wide summit caldera include a lake of meltwater, remnants of \n",
      "\n",
      "the snow that covers the peak for most of the year. The straight-down (nadir) satellite view can make it difficult to determine which \n",
      "\n",
      "part of the landscape stands taller than the other, a phenomenon known as relief inversion.\n",
      "\n",
      "On the next page, you can see how a layer of low clouds and fog obscures the lower elevations of the islands and the sea surface. \n",
      "\n",
      "But these clouds also hint at the complicated airflow patterns around and through the islands. \n",
      "\n",
      "Locations: ['Four Mountains', 'Pacific Ocean', 'Islands of the Four Mountains', 'Aleutian Island', 'peaks', 'slopes', 'seafloor', 'Carlisle', 'Cleveland', 'Herbert', 'Tana', 'world', 'volcanoes', 'Herbert Island', 'stratovolcano', 'island', 'summit caldera', 'landscape', 'islands', 'sea surface']\n",
      "Score: 0.03226646035909653 \n",
      "\n",
      "Title: page-8.pdf \n",
      "\n",
      "Chunk: than just \n",
      "\n",
      "blues and whites—are spread across the pages of this book.\n",
      "\n",
      "We chose these images because they inspire. They tell a story \n",
      "\n",
      "of a 4.5-billion-year-old planet where there is always something \n",
      "\n",
      "new to see. They tell a story of land, wind, water, ice, and air \n",
      "\n",
      "as they can only be viewed from above. They show us that no \n",
      "\n",
      "matter what the human mind can imagine, no matter what the \n",
      "\n",
      "artist can conceive, there are few things more fantastic and \n",
      "\n",
      "inspiring than the world as it already is. The truth of our planet  \n",
      "\n",
      "is just as compelling as any fiction.\n",
      "\n",
      "We hope you enjoy this satellite view of Earth.  \n",
      "\n",
      "It is your planet. It is NASA’s mission.\n",
      "\n",
      "Michael Carlowicz \n",
      "\n",
      "Locations: ['world', 'planet', 'Earth']\n",
      "Score: 0.032258063554763794 \n",
      "\n",
      "Title: page-8.pdf \n",
      "\n",
      "Chunk: This book stands at an intersection of science and art. From \n",
      "\n",
      "its origins, NASA has studied our planet in novel ways, using \n",
      "\n",
      "ingenious tools to study physical processes at work—from \n",
      "\n",
      "beneath the crust to the edge of the atmosphere. We look at it \n",
      "\n",
      "in macrocosm and microcosm, from the flow of one mountain \n",
      "\n",
      "stream to the flow of jet streams. Most of all, we look at Earth \n",
      "\n",
      "as a system, examining the cycles and processes—the water \n",
      "\n",
      "cycle, the carbon cycle, ocean circulation, the movement of \n",
      "\n",
      "heat—that interact and influence each other in a complex, \n",
      "\n",
      "dynamic dance across seasons and decades.\n",
      "\n",
      "We measure particles, gases, energy, and fluids moving in, on, \n",
      "\n",
      "and around Earth. And like artists, we study the light—how it \n",
      "\n",
      "bounces, reflects, refracts, and gets absorbed and changed. \n",
      "\n",
      "Understanding the light and the pictures it composes is no \n",
      "\n",
      "small feat, given the rivers of air and gas moving between our \n",
      "\n",
      "satellite eyes and the planet below.\n",
      "\n",
      "For all of the dynamism and detail we can observe from orbit, \n",
      "\n",
      "sometimes it is worth stepping back and simply admiring Earth. \n",
      "\n",
      "It is a beautiful, awe-inspiring place, and it is the only world \n",
      "\n",
      "most of us will ever know.\n",
      "\n",
      "NASA has a unique vantage point for observing the beauty and \n",
      "\n",
      "wonder of Earth and for making sense of it. Looking back from \n",
      "\n",
      "space, astronaut Edgar Mitchell once called Earth “a sparkling \n",
      "\n",
      "blue and white jewel,” and it does dazzle the eye. The planet’s \n",
      "\n",
      "palette of colors and textures and shapes—far more than just \n",
      "\n",
      "blues and whites—are spread across the pages of this book.\n",
      "\n",
      "We chose these images because they inspire. They tell a story \n",
      "\n",
      "of a 4.5-billion-year-old planet where there is always something \n",
      "\n",
      "new to see. They tell a story of land, wind, water, ice, and air \n",
      "\n",
      "as they can only be viewed from above. They show us that no \n",
      "\n",
      "matter what the human mind can imagine, no matter what the \n",
      "\n",
      "artist can conceive, there are few things more fantastic and \n",
      "\n",
      "inspiring than the world as it already is. \n",
      "\n",
      "Locations: ['planet', 'mountain', 'Earth', 'complex', 'world']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# User Query\n",
    "query = \"What can I see in the United States?\"  \n",
    "\n",
    "# Convert query into vector form\n",
    "vector_query = VectorizableTextQuery(text=query, \n",
    "                                     k_nearest_neighbors=50, \n",
    "                                     fields=\"text_vector\",\n",
    "                                     weight=1)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  #This performs a full-text search using the query\n",
    "    vector_queries= [vector_query], #This adds a vector search component\n",
    "    select=[\"title\",\"chunk\",\"locations\"], #Specify fields to be return in the result\n",
    "    top=3\n",
    ") \n",
    "\n",
    "for result in results:  \n",
    "    print(f\"Score: {result['@search.score']} \\n\")\n",
    "    print(f\"Title: {result['title']} \\n\")\n",
    "    print(f\"Chunk: {result['chunk']} \\n\")\n",
    "    print(f\"Locations: {result['locations']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Semantic Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.16025641560554504 \n",
      "\n",
      "Title: page-19.pdf \n",
      "\n",
      "Chunk: A\n",
      "T\n",
      "\n",
      "M\n",
      "O\n",
      "\n",
      "S\n",
      "P\n",
      "\n",
      "H\n",
      "E\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "12\n",
      "\n",
      "Punching Holes in the Sky\n",
      "United States\n",
      "\n",
      "In December 2009, the Landsat 5 satellite observed this extraordinary example of “hole-punch clouds” over West Virginia.  \n",
      "\n",
      "This strange phenomenon results from a combination of cold temperatures, air traffic, and atmospheric instability.\n",
      "\n",
      "If you were to look from below, it would appear as if part of the cloud were falling out of the sky. As it turns out, that is actually what \n",
      "\n",
      "is happening. The clouds are initially composed of liquid drops at a super-cooled temperature below 0° Celsius. As an airplane \n",
      "\n",
      "passes through a cloud, particles in the exhaust can create a disturbance that triggers freezing. Ice particles then quickly grow at the \n",
      "\n",
      "expense of water droplets. Eventually, the ice crystals in these patches of clouds grow large enough that they literally fall out of the \n",
      "\n",
      "sky, earning hole-punch clouds their alternate name: “fallstreak holes.”\n",
      "\n",
      "In this false-color image, pink and faint blue areas are typical water-rich clouds, and bright cyan areas are ice clouds.  \n",
      "\n",
      "The hole-punch areas appear dark around the ice clouds. \n",
      "\n",
      "Locations: ['Sky', 'United States', 'West Virginia', 'sky', 'hole-punch clouds', 'fallstreak holes']\n",
      "Score: 0.03226646035909653 \n",
      "\n",
      "Title: page-8.pdf \n",
      "\n",
      "Chunk: than just \n",
      "\n",
      "blues and whites—are spread across the pages of this book.\n",
      "\n",
      "We chose these images because they inspire. They tell a story \n",
      "\n",
      "of a 4.5-billion-year-old planet where there is always something \n",
      "\n",
      "new to see. They tell a story of land, wind, water, ice, and air \n",
      "\n",
      "as they can only be viewed from above. They show us that no \n",
      "\n",
      "matter what the human mind can imagine, no matter what the \n",
      "\n",
      "artist can conceive, there are few things more fantastic and \n",
      "\n",
      "inspiring than the world as it already is. The truth of our planet  \n",
      "\n",
      "is just as compelling as any fiction.\n",
      "\n",
      "We hope you enjoy this satellite view of Earth.  \n",
      "\n",
      "It is your planet. It is NASA’s mission.\n",
      "\n",
      "Michael Carlowicz \n",
      "\n",
      "Locations: ['world', 'planet', 'Earth']\n",
      "Score: 0.028985507786273956 \n",
      "\n",
      "Title: page-7.pdf \n",
      "\n",
      "Chunk: F\n",
      "o\n",
      "\n",
      "R\n",
      "e\n",
      "\n",
      "w\n",
      "o\n",
      "\n",
      "R\n",
      "d\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "vi\n",
      "\n",
      "Foreword\n",
      "\n",
      "of all celestial bodies within reach or view, as far as we can \n",
      "\n",
      "see, out to the edge, the most wonderful and marvelous and \n",
      "\n",
      "mysterious is turning out to be our own planet earth. There is \n",
      "\n",
      "nothing to match it anywhere, not yet anyway. \n",
      "\n",
      "—Lewis Thomas \n",
      "\n",
      "Sixty years ago, with the launch of Explorer 1, NASA made \n",
      "\n",
      "its first observations of Earth from space. Fifty years ago, \n",
      "\n",
      "astronauts left Earth orbit for the first time and looked back \n",
      "\n",
      "at our “blue marble.” All of these years later, as we send \n",
      "\n",
      "spacecraft and point our telescopes past the outer edges of  \n",
      "\n",
      "the solar system, as we study our planetary neighbors and  \n",
      "\n",
      "our Sun in exquisite detail, there remains much to see and \n",
      "\n",
      "explore at home.\n",
      "\n",
      "We are still just getting to know Earth through the tools of \n",
      "\n",
      "science. For centuries, painters, poets, philosophers, and \n",
      "\n",
      "photographers have sought to teach us something about our \n",
      "\n",
      "home through their art. \n",
      "\n",
      "Locations: ['earth', 'Earth', 'space', 'solar system', 'planetary', 'Sun', 'home']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# User Query\n",
    "query = \"What can I see in the United States?\"  \n",
    "\n",
    "# Convert query into vector form\n",
    "vector_query = VectorizableTextQuery(text=query, \n",
    "                                     k_nearest_neighbors=50, \n",
    "                                     fields=\"text_vector\",\n",
    "                                     weight=1)\n",
    "\n",
    "results = search_client.search(\n",
    "    query_type=\"semantic\", \n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    scoring_profile=\"my-scoring-profile\",\n",
    "    scoring_parameters=[\"tags-beach, 'United States'\"], \n",
    "    search_text=query,\n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\",\"chunk\",\"locations\"],\n",
    "    top=3,\n",
    ")\n",
    "\n",
    "# Sort the results by score in descending order\n",
    "sorted_results = sorted(results, key=lambda x: x['@search.score'], reverse=True)\n",
    "\n",
    "for result in sorted_results:  \n",
    "    print(f\"Score: {result['@search.score']} \\n\")\n",
    "    print(f\"Title: {result['title']} \\n\")\n",
    "    print(f\"Chunk: {result['chunk']} \\n\")\n",
    "    print(f\"Locations: {result['locations']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send query results to a language model to generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    # to get version: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key)\n",
    "\n",
    "# Provide instructions to the model\n",
    "SYSTEM_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided in this prompt.\n",
    "If the answer is longer than 3 sentences, provide a decently formatted summary in bullet points.\n",
    "Answer ONLY with the facts listed in the list of sources below. Cite your source when you answer the question\n",
    "If there isn't enough information in the sources, say you don't know.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the United States, you can observe the following phenomena:\n",
      "\n",
      "- **Hole-punch clouds**: This is an extraordinary phenomenon observed in December 2009 over West Virginia, where parts of the cloud appear to be falling out of the sky. It occurs due to cold temperatures, air traffic, and atmospheric instability, leading to the formation of ice crystals in super-cooled cloud areas (source: page-19.pdf).\n",
      "\n",
      "- **Ship tracks**: These are cloud trails left by ships steaming across the Pacific Ocean, observed off the coast of California. They result from water vapor condensing around tiny particles of pollution from ship exhaust, creating bright, narrow clouds that can stretch for many kilometers (source: page-31.pdf). \n",
      "\n",
      "Overall, the United States offers a range of fascinating atmospheric phenomena that showcase the unique interactions between human activity and natural processes.\n"
     ]
    }
   ],
   "source": [
    "# User Query\n",
    "query = \"What can I see in the United States?\"  \n",
    "\n",
    "# Convert query into vector form\n",
    "vector_query = VectorizableTextQuery(text=query, \n",
    "                                     k_nearest_neighbors=50, \n",
    "                                     fields=\"text_vector\",\n",
    "                                     weight=1)\n",
    "\n",
    "results = search_client.search(\n",
    "    query_type=\"semantic\", \n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    search_text=query,\n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\",\"chunk\",\"locations\"],\n",
    "    top=5,\n",
    ")\n",
    "\n",
    "# Use a unique separator to make the sources distinct. \n",
    "# We chose repeated equal signs (=) followed by a newline because it's unlikely the source documents contain this sequence.\n",
    "sources_formatted = \"=================\\n\".join([f'TITLE: {document[\"title\"]}, CONTENT: {document[\"chunk\"]}, LOCATIONS: {document[\"locations\"]}' for document in results])\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SYSTEM_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=azure_openai_deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_response(query):\n",
    "    \"\"\"\n",
    "    Function to perform RAG using Azure AI Search and Azure OpenAI.\n",
    "    \"\"\"\n",
    "    # Convert query into vector form\n",
    "    vector_query = VectorizableTextQuery(text=query, \n",
    "                                         k_nearest_neighbors=50, \n",
    "                                         fields=\"text_vector\",\n",
    "                                         weight=1)\n",
    "\n",
    "    results = search_client.search(\n",
    "        query_type=\"semantic\", \n",
    "        semantic_configuration_name='my-semantic-config',\n",
    "        search_text=query,\n",
    "        vector_queries= [vector_query],\n",
    "        select=[\"title\",\"chunk\",\"locations\"],\n",
    "        top=5,\n",
    "    )\n",
    "\n",
    "    sources_formatted = \"=================\\n\".join([f'TITLE: {document[\"title\"]}, CONTENT: {document[\"chunk\"]}, LOCATIONS: {document[\"locations\"]}' for document in results])\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": SYSTEM_PROMPT.format(query=query, sources=sources_formatted)\n",
    "            }\n",
    "        ],\n",
    "        model=azure_openai_deployment\n",
    "    )\n",
    "\n",
    "    return print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Valleys, Peru**: The valleys along Peru’s southern coast are some of the deepest on Earth and are frequently filled with clouds. The Yauca and Acarí rivers empty into the Pacific Ocean through these cloud-filled canyons (source: page-15.pdf).\n",
      "  \n",
      "- **Islands of the Four Mountains**: Located in the Aleutian Island chain, they consist of volcanoes such as Carlisle, Cleveland, Herbert, and Tana. These remote islands have rarely been studied and are characterized by cloud and fog obscuring parts of their landscape (source: page-35.pdf).\n",
      "  \n",
      "- **Glories and Vortices**: Over the Pacific Ocean, a layer of stratocumulus clouds can create a glory phenomenon, which is a rainbow-like visual effect. Additionally, swirling von Kármán vortices can be observed near islands like Guadalupe (source: page-17.pdf).\n",
      "\n",
      "- **Ship Tracks**: Ships traveling across the Pacific Ocean leave bright cloud trails known as ship tracks. These clouds are formed from water vapor condensing around pollution particles from ship exhaust and can extend for hundreds of kilometers (source: page-31.pdf).\n",
      "\n",
      "- **Hawaii**: The island of Hawaii is generally safe from hurricanes, with historical data showing no direct hits since 1949, despite nearby storms like Hurricanes Madeline and Lester in 2016 (source: page-27.pdf).\n"
     ]
    }
   ],
   "source": [
    "query = \"Mention interesting places in pacific ocean?\"\n",
    "rag_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, the following places can be visited in Africa:\n",
      "\n",
      "- **Madagascar**: Known for its tropical rainforests and the Mania River, which showcases unique cloud formation due to the moist conditions. (Source: page-45.pdf)\n",
      "- **Mauritania**: This region is noted for its unique atmospheric phenomena where air masses from Africa interact with the Atlantic Ocean, creating wave structures. (Source: page-23.pdf)\n",
      "\n",
      "These locations highlight the diverse environments and natural phenomena that can be explored in Africa.\n"
     ]
    }
   ],
   "source": [
    "query = \"What places can be visited in the Africa?\"\n",
    "rag_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"What places can be visited in New Zealand?\"\n",
    "rag_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
