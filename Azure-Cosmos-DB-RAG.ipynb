{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG solution using Azure Cosmos DB\n",
    "\n",
    "Steps in this notebook:\n",
    "1. Setup Azure Cosmos DB - database, container, policies (vector embedding, full text search, and indexing)\n",
    "2. Create embeddings\n",
    "3. Upload data to the container\n",
    "4. Send a query to the search engine to check results\n",
    "5. Send query results to a language model to generate response\n",
    "\n",
    "Note: Steps 1-4: Done during initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure configurations\n",
    "\n",
    "You always need to run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_openai_embeddings_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "azure_openai_api_version = \"2024-12-01-preview\"\n",
    "\n",
    "azure_cosmosdb_endpoint = os.getenv(\"AZURE_COSMOSDB_ENDPOINT\")\n",
    "azure_cosmosdb_key = os.getenv(\"AZURE_COSMOSDB_KEY\")\n",
    "azure_cosmosdb_database = \"azureservicesdatabase\"\n",
    "azure_cosmosdb_container = \"azureservicescontainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container with id 'azureservicescontainer' created\n"
     ]
    }
   ],
   "source": [
    "from azure.cosmos import CosmosClient\n",
    "from azure.cosmos import PartitionKey, exceptions\n",
    "\n",
    "# Setup the connection\n",
    "cosmos_client = CosmosClient(url=azure_cosmosdb_endpoint, credential=azure_cosmosdb_key)\n",
    "\n",
    "# Create database\n",
    "db = cosmos_client.create_database_if_not_exists(id=azure_cosmosdb_database)\n",
    "\n",
    "# Author the vector embedding policy\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\":\"/title_vector\",\n",
    "            \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"dotproduct\",\n",
    "            \"dimensions\":1536\n",
    "        },\n",
    "        {\n",
    "            \"path\":\"/content_vector\",\n",
    "            \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"cosine\",\n",
    "            \"dimensions\":1536\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "full_text_policy = {\n",
    "   \"defaultLanguage\": \"en-US\",\n",
    "   \"fullTextPaths\": [\n",
    "       {\n",
    "           \"path\": \"/title\",\n",
    "           \"language\": \"en-US\"\n",
    "       },\n",
    "       {\n",
    "           \"path\": \"/content\",\n",
    "           \"language\": \"en-US\"\n",
    "       },\n",
    "       {\n",
    "           \"path\": \"/category\",\n",
    "           \"language\": \"en-US\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "\n",
    "# Add vector indexes to indexing policy\n",
    "indexing_policy = {\n",
    "    \"includedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"excludedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/\\\"_etag\\\"/?\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/title_vector/*\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/content_vector/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"fullTextIndexes\": [\n",
    "        {\n",
    "            \"path\": \"/title\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/content\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/category\"\n",
    "        }\n",
    "    ],\n",
    "    \"vectorIndexes\": [\n",
    "        {\"path\": \"/title_vector\",\n",
    "         \"type\": \"quantizedFlat\"\n",
    "        },\n",
    "        {\"path\": \"/content_vector\",\n",
    "         \"type\": \"quantizedFlat\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:    \n",
    "    container = db.create_container_if_not_exists(\n",
    "                    id=azure_cosmosdb_container,\n",
    "                    partition_key=PartitionKey(path='/id', kind='Hash'),\n",
    "                    indexing_policy=indexing_policy,\n",
    "                    vector_embedding_policy=vector_embedding_policy,\n",
    "                    full_text_policy=full_text_policy)\n",
    "\n",
    "    print('Container with id \\'{0}\\' created'.format(container.id))\n",
    "\n",
    "except exceptions.CosmosResourceExistsError:\n",
    "    print('A container with id \\'{0}\\' already exists'.format(container.id))\n",
    "\n",
    "container = db.get_container_client(azure_cosmosdb_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings separately\n",
    "\n",
    "We are computing the embeddings manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "# Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    # to get version: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=azure_openai_embeddings_deployment,\n",
    "    api_key=azure_openai_key)\n",
    "\n",
    "# Read the AzureServices.json\n",
    "path = os.path.join('Data/azureservices/', 'AzureServices.json')\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "titles = [item['title'] for item in input_data]\n",
    "content = [item['content'] for item in input_data]\n",
    "title_response = openai_client.embeddings.create(input=titles, \n",
    "                                                 model=azure_openai_embeddings_deployment, \n",
    "                                                 dimensions=1536)\n",
    "title_embeddings = [item.embedding for item in title_response.data]\n",
    "content_response = openai_client.embeddings.create(input=content, \n",
    "                                                   model=azure_openai_embeddings_deployment, \n",
    "                                                   dimensions=1536)\n",
    "content_embeddings = [item.embedding for item in content_response.data]\n",
    "\n",
    "# Generate embeddings for title and content fields\n",
    "for i, item in enumerate(input_data):\n",
    "    title = item['title']\n",
    "    content = item['content']\n",
    "    item['title_vector'] = title_embeddings[i]\n",
    "    item['content_vector'] = content_embeddings[i]\n",
    "\n",
    "# Output embeddings to docVectors.json file\n",
    "output_path = os.path.join('Data/output', 'cosmos_docVectors.json')\n",
    "output_directory = os.path.dirname(output_path)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing item  1\n",
      "writing item  2\n",
      "writing item  3\n",
      "writing item  4\n",
      "writing item  5\n",
      "writing item  6\n",
      "writing item  7\n",
      "writing item  8\n",
      "writing item  9\n",
      "writing item  10\n",
      "writing item  11\n",
      "writing item  12\n",
      "writing item  13\n",
      "writing item  14\n",
      "writing item  15\n",
      "writing item  16\n",
      "writing item  17\n",
      "writing item  18\n",
      "writing item  19\n",
      "writing item  20\n",
      "writing item  21\n",
      "writing item  22\n",
      "writing item  23\n",
      "writing item  24\n",
      "writing item  25\n",
      "writing item  26\n",
      "writing item  27\n",
      "writing item  28\n",
      "writing item  29\n",
      "writing item  30\n",
      "writing item  31\n",
      "writing item  32\n",
      "writing item  33\n",
      "writing item  34\n",
      "writing item  35\n",
      "writing item  36\n",
      "writing item  37\n",
      "writing item  38\n",
      "writing item  39\n",
      "writing item  40\n",
      "writing item  41\n",
      "writing item  42\n",
      "writing item  43\n",
      "writing item  44\n",
      "writing item  45\n",
      "writing item  46\n",
      "writing item  47\n",
      "writing item  48\n",
      "writing item  49\n",
      "writing item  50\n",
      "writing item  51\n",
      "writing item  52\n",
      "writing item  53\n",
      "writing item  54\n",
      "writing item  55\n",
      "writing item  56\n",
      "writing item  57\n",
      "writing item  58\n",
      "writing item  59\n",
      "writing item  60\n",
      "writing item  61\n",
      "writing item  62\n",
      "writing item  63\n",
      "writing item  64\n",
      "writing item  65\n",
      "writing item  66\n",
      "writing item  67\n",
      "writing item  68\n",
      "writing item  69\n",
      "writing item  70\n",
      "writing item  71\n",
      "writing item  72\n",
      "writing item  73\n",
      "writing item  74\n",
      "writing item  75\n",
      "writing item  76\n",
      "writing item  77\n",
      "writing item  78\n",
      "writing item  79\n",
      "writing item  80\n",
      "writing item  81\n",
      "writing item  82\n",
      "writing item  83\n",
      "writing item  84\n",
      "writing item  85\n",
      "writing item  86\n",
      "writing item  87\n",
      "writing item  88\n",
      "writing item  89\n",
      "writing item  90\n",
      "writing item  91\n",
      "writing item  92\n",
      "writing item  93\n",
      "writing item  94\n",
      "writing item  95\n",
      "writing item  96\n",
      "writing item  97\n",
      "writing item  98\n",
      "writing item  99\n",
      "writing item  100\n",
      "writing item  101\n",
      "writing item  102\n",
      "writing item  103\n",
      "writing item  104\n",
      "writing item  105\n",
      "writing item  106\n",
      "writing item  107\n",
      "writing item  108\n"
     ]
    }
   ],
   "source": [
    "with open('Data/output/cosmos_docVectors.json') as f:\n",
    "   data = json.load(f)\n",
    "\n",
    "container_client = db.get_container_client(azure_cosmosdb_container)\n",
    "\n",
    "for item in data:\n",
    "    print(\"writing item \", item['id'])\n",
    "    container_client.upsert_item(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to assist with vector search\n",
    "def vector_search(user_query, num_results):\n",
    "\n",
    "    # Setup the connection\n",
    "    cosmos_client = CosmosClient(url=azure_cosmosdb_endpoint, credential=azure_cosmosdb_key)\n",
    "    database = cosmos_client.get_database_client(azure_cosmosdb_database)\n",
    "    container = database.get_container_client(azure_cosmosdb_container)\n",
    "\n",
    "    # Azure OpenAI client\n",
    "    openai_client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=azure_openai_embeddings_deployment,\n",
    "    api_key=azure_openai_key)\n",
    "\n",
    "    query_embedding = openai_client.embeddings.create(input=user_query, \n",
    "                                               model=azure_openai_embeddings_deployment, \n",
    "                                               dimensions=1536)\n",
    "    query_vector = query_embedding.data[0].embedding\n",
    "\n",
    "    results = container.query_items(\n",
    "            query='''\n",
    "                SELECT TOP @num_results c.id, c.title, c.content, c.category, VectorDistance(c.content_vector, @query_vector) AS SimilarityScore \n",
    "                FROM c\n",
    "                ORDER BY VectorDistance(c.content_vector, @query_vector)\n",
    "            ''',\n",
    "            parameters=[\n",
    "                {\"name\": \"@query_vector\", \"value\": query_vector}, \n",
    "                {\"name\": \"@num_results\", \"value\": num_results} \n",
    "            ],\n",
    "            enable_cross_partition_query=True)\n",
    "\n",
    "    # Extract the necessary information from the results\n",
    "    formatted_results = []\n",
    "    for document in results:\n",
    "        score = document.pop('SimilarityScore')\n",
    "        formatted_result = {\n",
    "            'SimilarityScore': score,\n",
    "            'document': document\n",
    "        }\n",
    "        formatted_results.append(formatted_result)\n",
    "    # Sort the results by similarity score in descending order\n",
    "    formatted_results.sort(key=lambda x: x['SimilarityScore'], reverse=True)\n",
    "\n",
    "    return formatted_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Query Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.8001937493740583\n",
      "ID: 6\n",
      "Title: Azure Cosmos DB\n",
      "Category: Databases\n",
      "Content: Azure Cosmos DB is a fully managed, globally distributed, multi-model database service designed for building highly responsive and scalable applications. It offers turnkey global distribution, automatic and instant scalability, and guarantees low latency, high availability, and consistency. Cosmos DB supports popular NoSQL APIs, including MongoDB, Cassandra, Gremlin, and Azure Table Storage. You can build globally distributed applications with ease, without having to deal with complex configuration and capacity planning. Data stored in Cosmos DB is automatically indexed, enabling you to query your data with SQL, JavaScript, or other supported query languages.\n",
      "\n",
      "Similarity Score: 0.7894223494597573\n",
      "ID: 70\n",
      "Title: Azure Cosmos DB\n",
      "Category: Databases\n",
      "Content: Azure Cosmos DB is a globally distributed, multi-model database service that enables you to build and manage NoSQL applications in Azure. It provides features like automatic scaling, low-latency access, and multi-master replication. Cosmos DB supports various data models, such as key-value, document, graph, and column-family. You can use Azure Cosmos DB to build globally distributed applications, ensure high availability and performance, and manage your data at scale. It also integrates with other Azure services, such as Azure Functions and Azure App Service.\n",
      "\n",
      "Similarity Score: 0.5432020865937301\n",
      "ID: 52\n",
      "Title: Azure Table Storage\n",
      "Category: Storage\n",
      "Content: Azure Table Storage is a fully managed, NoSQL datastore that enables you to store and query large amounts of structured, non-relational data. It provides features like automatic scaling, schema-less design, and a RESTful API. Table Storage supports various data types, such as strings, numbers, and booleans. You can use Azure Table Storage to store and manage your data, build scalable applications, and reduce the cost of your storage. It also integrates with other Azure services, such as Azure Functions and Azure Cosmos DB.\n",
      "\n",
      "Similarity Score: 0.5271928113909654\n",
      "ID: 5\n",
      "Title: Azure SQL Database\n",
      "Category: Databases\n",
      "Content: Azure SQL Database is a fully managed relational database service based on the latest stable version of Microsoft SQL Server. It offers built-in intelligence that learns your application patterns and adapts to maximize performance, reliability, and data protection. SQL Database supports elastic scaling, allowing you to dynamically adjust resources to match your workload. It provides advanced security features, such as encryption, auditing, and threat detection. You can migrate your existing SQL Server databases to Azure SQL Database with minimal downtime.\n",
      "\n",
      "Similarity Score: 0.5225920362273674\n",
      "ID: 40\n",
      "Title: Azure Cognitive Search\n",
      "Category: AI + Machine Learning\n",
      "Content: Azure Cognitive Search is a fully managed search-as-a-service that enables you to build rich search experiences for your applications. It provides features like full-text search, faceted navigation, and filters. Azure Cognitive Search supports various data sources, such as Azure SQL Database, Azure Blob Storage, and Azure Cosmos DB. You can use Azure Cognitive Search to index your data, create custom scoring profiles, and integrate with other Azure services. It also integrates with other Azure services, such as Azure Cognitive Services and Azure Machine Learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Azure cosmos db?\"\n",
    "results = vector_search(query, 5)\n",
    "\n",
    "for item in results:\n",
    "    print(f\"Similarity Score: {item['SimilarityScore']}\")\n",
    "    print(f\"ID: {item['document']['id']}\")\n",
    "    print(f\"Title: {item['document']['title']}\")\n",
    "    print(f\"Category: {item['document']['category']}\")\n",
    "    print(f\"Content: {item['document']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send query results to a language model to generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Azure Firewall is a managed, cloud-based network security service.\n",
      "- It protects Azure Virtual Network resources and offers features such as:\n",
      "  - Stateful packet inspection\n",
      "  - Application filtering\n",
      "  - Threat intelligence\n",
      "- It supports various network protocols, including TCP, UDP, and ICMP.\n",
      "- Azure Firewall enables the creation and enforcement of network security policies, prevents unauthorized access, and protects applications and data.\n",
      "- It integrates with other Azure services like Azure Monitor and Azure Security Center (Source: Azure Firewall).\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key)\n",
    "\n",
    "# Provide instructions to the model\n",
    "SYSTEM_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided below.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources below. Cite your source when you answer the question\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# User Query\n",
    "query = \"What is Azure Firewall?\"\n",
    "\n",
    "results = vector_search(query, 5)\n",
    "\n",
    "# Use a unique separator to make the sources distinct. \n",
    "# We chose repeated equal signs (=) followed by a newline because it's unlikely the source documents contain this sequence.\n",
    "sources_formatted = \"=================\\n\".join([f\"TITLE: {document['document']['title']}, CONTENT: {document['document']['content']}, CATEGORY: {document['document']['category']}\" for document in results])\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SYSTEM_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=azure_openai_deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"id\": \"24\",\n",
      "  \"title\": \"Azure Firewall\",\n",
      "  \"category\": \"Security\",\n",
      "  \"content\": \"Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It provides features like stateful packet inspection, application filtering, and threat intelligence. Firewall supports various network protocols, including TCP, UDP, and ICMP. You can use Azure Firewall to create and enforce network security policies, prevent unauthorized access, and protect your applications and data. It also integrates with other Azure services, such as Azure Monitor and Azure Security Center.\"\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Simple function to assist with full text search\n",
    "def full_text_search(user_query, num_results):\n",
    "\n",
    "    # Setup the connection\n",
    "    cosmos_client = CosmosClient(url=azure_cosmosdb_endpoint, credential=azure_cosmosdb_key)\n",
    "    database = cosmos_client.get_database_client(azure_cosmosdb_database)\n",
    "    container = container = database.get_container_client(azure_cosmosdb_container)\n",
    "\n",
    "    # Build the query with str.format() method\n",
    "    query = '''\n",
    "        SELECT TOP {0} c.id, c.title, c.category, c.content\n",
    "        FROM c\n",
    "        WHERE FullTextContains(c.title, '{1}')\n",
    "    '''.format(num_results, user_query)\n",
    "\n",
    "    results = container.query_items(\n",
    "            query=query,\n",
    "            enable_cross_partition_query=True)\n",
    "\n",
    "    items = [item for item in results]\n",
    "    \n",
    "    output = json.dumps(items, indent=True)\n",
    "    print(output)\n",
    "      \n",
    "full_text_search(\"Firewall\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"id\": \"18\",\n",
      "  \"title\": \"Azure Synapse Analytics\",\n",
      "  \"category\": \"Analytics\",\n",
      "  \"content\": \"Azure Synapse Analytics is an integrated analytics service that brings together big data and data warehousing. It enables you to ingest, prepare, manage, and serve data for immediate business intelligence and machine learning needs. Synapse Analytics provides a unified workspace for data engineers, data scientists, and business analysts to collaborate and build solutions. It supports various data sources, including Azure Data Lake Storage, Azure Blob Storage, and Azure Cosmos DB. You can use Synapse Analytics with other Azure services, such as Azure Machine Learning and Power BI.\"\n",
      " },\n",
      " {\n",
      "  \"id\": \"71\",\n",
      "  \"title\": \"Azure SQL Data Warehouse\",\n",
      "  \"category\": \"Databases\",\n",
      "  \"content\": \"Azure SQL Data Warehouse is a fully managed, petabyte-scale cloud data warehouse service that enables you to store and analyze your structured and semi-structured data. It provides features like automatic scaling, data movement, and integration with Azure Machine Learning. SQL Data Warehouse supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure SQL Database. You can use Azure SQL Data Warehouse to build data lakes, develop big data analytics solutions, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory.\"\n",
      " },\n",
      " {\n",
      "  \"id\": \"83\",\n",
      "  \"title\": \"Azure Databricks\",\n",
      "  \"category\": \"Analytics\",\n",
      "  \"content\": \"Azure Databricks is a fully managed, Apache Spark-based analytics platform that enables you to process and analyze large volumes of data. It provides features like collaborative notebooks, job scheduling, and integration with Azure Machine Learning. Databricks supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can use Azure Databricks to build data lakes, develop big data analytics solutions, and train machine learning models. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory.\"\n",
      " },\n",
      " {\n",
      "  \"id\": \"107\",\n",
      "  \"title\": \"Azure Data Bricks\",\n",
      "  \"category\": \"Analytics\",\n",
      "  \"content\": \"Azure Data Bricks is a fully managed, Apache Spark-based analytics platform that enables you to process and analyze large volumes of data. It provides features like collaborative notebooks, job scheduling, and integration with Azure Machine Learning. Data Bricks supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can use Azure Data Bricks to build data lakes, develop big data analytics solutions, and train machine learning models. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory.\"\n",
      " },\n",
      " {\n",
      "  \"id\": \"12\",\n",
      "  \"title\": \"Azure Databricks\",\n",
      "  \"category\": \"Analytics\",\n",
      "  \"content\": \"Azure Databricks is an Apache Spark-based analytics platform optimized for the Azure cloud. It provides a collaborative workspace for data scientists, engineers, and business users to process, analyze, and visualize big data. Databricks supports multiple programming languages, including Python, Scala, R, and SQL. It offers built-in integration with Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can also use Databricks to train and deploy machine learning models, and integrate with Azure Machine Learning.\"\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Simple function to assist with full text search\n",
    "def hybrid_search(user_query, num_results):\n",
    "\n",
    "    # Setup the connection\n",
    "    cosmos_client = CosmosClient(url=azure_cosmosdb_endpoint, credential=azure_cosmosdb_key)\n",
    "    database = cosmos_client.get_database_client(azure_cosmosdb_database)\n",
    "    container = container = database.get_container_client(azure_cosmosdb_container)\n",
    "\n",
    "    # Azure OpenAI client\n",
    "    openai_client = AzureOpenAI(\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_endpoint=azure_openai_endpoint,\n",
    "        azure_deployment=azure_openai_embeddings_deployment,\n",
    "        api_key=azure_openai_key)\n",
    "\n",
    "    response = openai_client.embeddings.create(input=user_query, \n",
    "                                               model=azure_openai_embeddings_deployment, \n",
    "                                               dimensions=1536)\n",
    "    query_vector = response.data[0].embedding\n",
    "\n",
    "\n",
    "    # Build the query with str.format() method\n",
    "    query = '''\n",
    "        SELECT TOP {0} c.id, c.title, c.category, c.content\n",
    "        FROM c\n",
    "        ORDER BY RANK RRF \n",
    "            (VectorDistance(c.content_vector, {1}), FullTextScore(c.title, '{2}'))\n",
    "    '''.format(num_results, query_vector, user_query)\n",
    "\n",
    "    results = container.query_items(\n",
    "            query=query,\n",
    "            enable_cross_partition_query=True)\n",
    "\n",
    "    items = [item for item in results]\n",
    "    \n",
    "    output = json.dumps(items, indent=True)\n",
    "    print(output)\n",
    "    return items\n",
    "      \n",
    "results = hybrid_search(\"what is azure synapse\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '18', 'title': 'Azure Synapse Analytics', 'category': 'Analytics', 'content': 'Azure Synapse Analytics is an integrated analytics service that brings together big data and data warehousing. It enables you to ingest, prepare, manage, and serve data for immediate business intelligence and machine learning needs. Synapse Analytics provides a unified workspace for data engineers, data scientists, and business analysts to collaborate and build solutions. It supports various data sources, including Azure Data Lake Storage, Azure Blob Storage, and Azure Cosmos DB. You can use Synapse Analytics with other Azure services, such as Azure Machine Learning and Power BI.'}\n",
      "{'id': '71', 'title': 'Azure SQL Data Warehouse', 'category': 'Databases', 'content': 'Azure SQL Data Warehouse is a fully managed, petabyte-scale cloud data warehouse service that enables you to store and analyze your structured and semi-structured data. It provides features like automatic scaling, data movement, and integration with Azure Machine Learning. SQL Data Warehouse supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure SQL Database. You can use Azure SQL Data Warehouse to build data lakes, develop big data analytics solutions, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory.'}\n",
      "{'id': '83', 'title': 'Azure Databricks', 'category': 'Analytics', 'content': 'Azure Databricks is a fully managed, Apache Spark-based analytics platform that enables you to process and analyze large volumes of data. It provides features like collaborative notebooks, job scheduling, and integration with Azure Machine Learning. Databricks supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can use Azure Databricks to build data lakes, develop big data analytics solutions, and train machine learning models. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory.'}\n",
      "{'id': '107', 'title': 'Azure Data Bricks', 'category': 'Analytics', 'content': 'Azure Data Bricks is a fully managed, Apache Spark-based analytics platform that enables you to process and analyze large volumes of data. It provides features like collaborative notebooks, job scheduling, and integration with Azure Machine Learning. Data Bricks supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can use Azure Data Bricks to build data lakes, develop big data analytics solutions, and train machine learning models. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory.'}\n",
      "{'id': '12', 'title': 'Azure Databricks', 'category': 'Analytics', 'content': 'Azure Databricks is an Apache Spark-based analytics platform optimized for the Azure cloud. It provides a collaborative workspace for data scientists, engineers, and business users to process, analyze, and visualize big data. Databricks supports multiple programming languages, including Python, Scala, R, and SQL. It offers built-in integration with Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can also use Databricks to train and deploy machine learning models, and integrate with Azure Machine Learning.'}\n"
     ]
    }
   ],
   "source": [
    "for item in results:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "sources_formatted = \"=================\\n\".join([f\"TITLE: {document['title']}, CONTENT: {document['content']}, CATEGORY: {document['category']}\" for document in results])\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SYSTEM_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=azure_openai_deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITLE: Azure Synapse Analytics, CONTENT: Azure Synapse Analytics is an integrated analytics service that brings together big data and data warehousing. It enables you to ingest, prepare, manage, and serve data for immediate business intelligence and machine learning needs. Synapse Analytics provides a unified workspace for data engineers, data scientists, and business analysts to collaborate and build solutions. It supports various data sources, including Azure Data Lake Storage, Azure Blob Storage, and Azure Cosmos DB. You can use Synapse Analytics with other Azure services, such as Azure Machine Learning and Power BI., CATEGORY: Analytics=================\\nTITLE: Azure SQL Data Warehouse, CONTENT: Azure SQL Data Warehouse is a fully managed, petabyte-scale cloud data warehouse service that enables you to store and analyze your structured and semi-structured data. It provides features like automatic scaling, data movement, and integration with Azure Machine Learning. SQL Data Warehouse supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure SQL Database. You can use Azure SQL Data Warehouse to build data lakes, develop big data analytics solutions, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory., CATEGORY: Databases=================\\nTITLE: Azure Databricks, CONTENT: Azure Databricks is a fully managed, Apache Spark-based analytics platform that enables you to process and analyze large volumes of data. It provides features like collaborative notebooks, job scheduling, and integration with Azure Machine Learning. Databricks supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can use Azure Databricks to build data lakes, develop big data analytics solutions, and train machine learning models. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory., CATEGORY: Analytics=================\\nTITLE: Azure Data Bricks, CONTENT: Azure Data Bricks is a fully managed, Apache Spark-based analytics platform that enables you to process and analyze large volumes of data. It provides features like collaborative notebooks, job scheduling, and integration with Azure Machine Learning. Data Bricks supports various data sources, such as Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can use Azure Data Bricks to build data lakes, develop big data analytics solutions, and train machine learning models. It also integrates with other Azure services, such as Azure Synapse Analytics and Azure Data Factory., CATEGORY: Analytics=================\\nTITLE: Azure Databricks, CONTENT: Azure Databricks is an Apache Spark-based analytics platform optimized for the Azure cloud. It provides a collaborative workspace for data scientists, engineers, and business users to process, analyze, and visualize big data. Databricks supports multiple programming languages, including Python, Scala, R, and SQL. It offers built-in integration with Azure Blob Storage, Azure Data Lake Storage, and Azure Cosmos DB. You can also use Databricks to train and deploy machine learning models, and integrate with Azure Machine Learning., CATEGORY: Analytics'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
