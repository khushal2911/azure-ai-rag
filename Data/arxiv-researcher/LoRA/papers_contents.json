{
  "2506.19852v1": "arXiv:2506.19852v1  [cs.CV]  24 Jun 2025\nRadial Attention: O(n log n) Sparse Attention\nwith Energy Decay for Long Video Generation\nXingyang Li\u2217 Muyang Li\u2217 Tianle Cai Haocheng Xi\nShuo Yang Yujun Lin Lvmin Zhang Songlin Yang Jinbo Hu\nKelly Peng Maneesh Agrawala Ion Stoica Kurt Keutzer Song Han\nMIT NVIDIA Princeton UC Berkeley Stanford First Intelligence\nhttps://github.com/mit-han-lab/radial-attention\n\u201cNothing spreads without loss; every signal, every influence, every attention \u2014\ndecays with distance.\u201d \u2014 Inspired by thermodynamic principles\nPre-Trained\nLoRA-Tuned Pre-Trained\nPrompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black \npurse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\nDense Attention    Latency: 1649s Radial Attention (Ours)    Latency: 876s (1.9\u00d7 Faster)    PSNR: 27.3 \n(a) 117 Frames (Default Length)\nDense Attention    Vision Reward: 0.054    Latency: 2895s Dense Attention+RIFLEx    Vision Reward: 0.037    Latency: 2895s\nDense Attention    Vision Reward: 0.133 \nTuning Cost: 746 GPU Hours      Latency: 2895s\nRadial Attention (Ours)    Vision Reward: 0.134 \nTuning Cost: 171 GPU Hours (4.4\u00d7 Fewer)     Latency: 781s (3.7\u00d7 Faster)\n(b) 509 Frames (4\u00d7 Extension)\nFigure 1: We present Radial Attention, a sparse attention mechanism with O(n log n) computational complexity.\nRadial Attention accelerates pre-trained HunyuanVideo [1] by 1.9\u00d7 at its default video length while maintaining\ncomparable video quality. When generating 4\u00d7 longer videos, it reduces tuning costs by up to 4.4\u00d7 and speeds\nup inference by up to 3.7\u00d7 versus dense attention.\nAbstract\nRecent advances in diffusion models have enabled high-quality video generation,\nbut the additional temporal dimension significantly increases computational costs,\nmaking training and inference on long videos prohibitively expensive. In this\npaper, we identify a phenomenon we term Spatiotemporal Energy Decay in video\ndiffusion models: post-softmax attention scores diminish as spatial and temporal\ndistance between tokens increase, akin to the physical decay of signal or waves\nover space and time in nature. Motivated by this, we propose Radial Attention, a\nscalable sparse attention mechanism with O(n log n) complexity that translates\nenergy decay into exponentially decaying compute density, which is significantly\nmore efficient than standard O(n2) dense attention and more expressive than linear\nattention. Specifically, Radial Attention employs a simple, static attention mask\nwhere each token attends to spatially nearby tokens, with the attention window size\n\u2217indicates equal contributions.\nPreprint. Under review.",
  "2506.19658v1": "arXiv:2506.19658v1  [cs.CV]  24 Jun 2025\nSAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set\nGuided Prompting\nYang Xing\nJ. Crayton Pruitt Family\nDepartment of\nBiomedical Engineering\nUniversity of Florida\nJiong Wu\nJ. Crayton Pruitt Family\nDepartment of\nBiomedical Engineering\nUniversity of Florida\nYuheng Bu\nDepartment of Electrical\n& Computer Engineering\nUniversity of Florida\nKuang Gong\nJ. Crayton Pruitt Family\nDepartment of\nBiomedical Engineering\nUniversity of Florida\nAbstract\u2014Although new vision foundation models such as Seg-\nment Anything Model 2 (SAM2) have significantly enhanced\nzero-shot image segmentation capabilities, reliance on human-\nprovided prompts poses significant challenges in adapting\nSAM2 to medical image segmentation tasks. Moreover, SAM2\u2019s\nperformance in medical image segmentation was limited by the\ndomain shift issue, since it was originally trained on natural\nimages and videos. To address these challenges, we proposed\nSAM2 with support-set guided prompting (SAM2-SGP), a\nframework that eliminated the need for manual prompts. The\nproposed model leveraged the memory mechanism of SAM2\nto generate pseudo-masks using image\u2013mask pairs from a\nsupport set via a Pseudo-mask Generation (PMG) module.\nWe further introduced a novel Pseudo-mask Attention (PMA)\nmodule, which used these pseudo-masks to automatically gen-\nerate bounding boxes and enhance localized feature extraction\nby guiding attention to relevant areas. Furthermore, a low-\nrank adaptation (LoRA) strategy was adopted to mitigate the\ndomain shift issue. The proposed framework was evaluated\non both 2D and 3D datasets across multiple medical imaging\nmodalities, including fundus photography, X-ray, computed\ntomography (CT), magnetic resonance imaging (MRI), positron\nemission tomography (PET), and ultrasound. The results\ndemonstrated a significant performance improvement over\nstate-of-the-art models, such as nnUNet and SwinUNet, as well\nas foundation models, such as SAM2 and MedSAM2, under-\nscoring the effectiveness of the proposed approach. Our code is\npublicly available at https://github.com/astlian9/SAM Support.\nIndex Terms \u2014Auto-prompting, Fine-tuning, Foundation\nModel, Medical Image Segmentation, SAM2.\n1. Introduction\nVision foundation models have demonstrated strong\nzero-shot capabilities across various applications, including\nmedical image segmentation [1]. Their impressive gener-\nalizability and few-shot learning capabilities make them\nattractive for adapting to downstream tasks, offering a\nmore efficient alternative to training task-specific models\nfrom scratch. The segment anything model (SAM) [2] is\na recently developed visual foundation model designed for\npromptable image segmentation, pretrained on over 1 billion\nmasks from 11 million natural images. Leveraging its large-\nscale training data and generalizable architecture, SAM ex-\nhibited strong zero-shot segmentation performance by using\nprompts as an extra input, such as a bounding box or\npositive and negative clicks, demonstrating exceptional gen-\neralization ability and establishing a new benchmark across\nvarious segmentation tasks [3], [4], [5]. Recent works also\ndemonstrated the strong performance of the SAM model\nwhen applied to downstream medical image segmentation\ntasks [6], [7], [8], [9], [10], [11].\nTo extend these capabilities to more complex scenar-\nios, the SAM2 model has been developed to expand the\nfunctionality of SAM to include video inputs [12]. This\nextension enabled SAM2 to process temporal sequences\nof images, making it suitable for tasks that required the\nunderstanding of spatial continuity over multiple frames.\nFine-tuning it on specific tasks [13], [14], [15] and directly\nevaluating it on few-shot segmentation [16], [17], [18] are\ntwo ongoing research topics of SAM2 in medical image\nsegmentation [19]. Although these SAM2-based methods\nrequired minimal or no training data, they still had notable\nlimitations. Firstly, their performance remained highly de-\npendent on user-provided high-quality instructions. Due to\nthis limitation, recent work on the SAM2 model focused\nmainly on interactive medical image segmentation. Further-\nmore, it was trained on natural images and videos and could\nface domain shift issues when applied to medical image\nsegmentation tasks.\nTo tackle the aforementioned limitations, we proposed\na novel model, SAM2 with support-set guided prompting\n(SAM2-SGP), for medical image segmentation. By incor-\nporating in-context learning with the memory mechanism\nof SAM2, SAM2-SGP could automatically generate high-\nquality prompts based on support sets. Specifically, the\nproposed SAM2-SGP model included a novel generator\nadapted from SAM2\u2019s memory mechanism to generate the\npseudo-mask from image-mask pairs of the support set.\nThese pseudo-masks were then used to compute bounding\nboxes, which could be used to generate prompt embeddings.",
  "2506.19306v1": "   [2024] Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) \nI/ITSEC [2024] Paper No. 24322 Page 1 of 13 \nAirway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze  Jean-Paul Ainam, Rahul Lora Cavuoto  CeMSIM, RPI University at Buffalo  Troy, New York Buffalo, NY  ainamj@rpi.edu, rahul@rpi.edu loracavu@buffalo.edu   Matthew Hackett, Jack Norfleet Suvranu De CCDC, Center STTC Florida State University Orlando, FL Tallahassee, FL matthew.g.hackett.civ@army.mil, jack.e.norfleet.civ@army.mil sde@eng.famu.fsu.edu   ABSTRACT  Airway management skills are critical in emergency medicine and are typically assessed through subjective evaluation, often failing to gauge competency in real-world scenarios. This paper proposes a machine learning-based approach for assessing airway skills, specifically endotracheal intubation (ETI), using human gaze data and video recordings. The proposed system leverages an attention mechanism guided by the human gaze to enhance the recognition of successful and unsuccessful ETI procedures. Visual masks were created from gaze points to guide the model in focusing on task-relevant areas, reducing irrelevant features. An autoencoder network extracts features from the videos, while an attention module generates attention from the visual masks, and a classifier outputs a classification score. This method, the first to use human gaze for ETI, demonstrates improved accuracy and efficiency over traditional methods. The integration of human gaze data not only enhances model performance but also offers a robust, objective assessment tool for clinical skills, particularly in high-stress environments such as military settings. The results show improvements in prediction accuracy, sensitivity, and trustworthiness, highlighting the potential for this approach to improve clinical training and patient outcomes in emergency medicine.     ABOUT THE AUTHORS  Jean-Paul Ainam, PhD is a research scientist at the Center for Modeling, Simulation & Imaging in Medicine, RPI, focusing on Machine learning applied to videos. His research interests include Generative Adversarial Networks, Scene understanding, and deep neural network architecture design for multi-view and multimodal data.  Rahul is an Assistant Professor in the Department of Biomedical Engineering at Rensselaer Polytechnic (RPI). He is actively engaged in interdisciplinary research that combines physics, biomedical imaging, and data-driven analytics for solving problems in healthcare with the eventual goal of reducing medical errors and advancing patient safety.   Lora Cavuoto, PhD is a Professor in the Department of Industrial and Systems Engineering at the University at Buffalo. Dr. Cavuoto's research focuses on improving occupational safety and productivity through ergonomic principles and advanced technologies.    Matthew Hackett, PhD, is a Technical Lead and Manager for various medical simulation programs, including virtual patients, medical holography, serious games, mobile applications, TCCC, and volumetric at the U.S. Army CCDC Soldier Center. Hacket\u2019s research is focused on healthcare simulation and training, particularly military healthcare and combat casualty care.  ",
  "2506.19072v1": "arXiv:2506.19072v1  [cs.CV]  23 Jun 2025\nHAWAII: Hierarchical Visual Knowledge Transfer for\nEfficient Vision-Language Models\nYimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki\nUniversity of Waterloo\nAbstract\nImproving the visual understanding ability of vision-language models (VLMs) is\ncrucial for enhancing their performance across various tasks. While using multiple\npretrained visual experts has shown great promise, it often incurs significant compu-\ntational costs during training and inference. To address this challenge, we propose\nHAWAII, a novel framework that distills knowledge from multiple visual experts\ninto a single vision encoder, enabling it to inherit the complementary strengths of\nseveral experts with minimal computational overhead. To mitigate conflicts among\ndifferent teachers and switch between different teacher-specific knowledge, instead\nof using a fixed set of adapters for multiple teachers, we propose to use teacher-\nspecific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each\nadapter is aligned with a specific teacher, avoiding noisy guidance during distil-\nlation. To enable efficient knowledge distillation, we propose fine-grained and\ncoarse-grained distillation. At the fine-grained level, token importance scores are\nemployed to emphasize the most informative tokens from each teacher adaptively.\nAt the coarse-grained level, we summarize the knowledge from multiple teachers\nand transfer it to the student using a set of general-knowledge LoRA adapters with\na router. Extensive experiments on various vision-language tasks demonstrate the\nsuperiority of HAWAII, compared to the popular open-source VLMs.\n1 Introduction\nVision-language models (VLMs) [1, 2] enable machines to perform complex reasoning over multi-\nmodal inputs by combining the powerful language reasoning capabilities of pretrained large language\nmodels (LLMs) [3, 4, 5] with the rich perceptual understanding offered by vision foundation mod-\nels [6, 7, 8]. These two components are connected through alignment modules, such as Q-Formers [9]\nor MLP projections [10], which map visual tokens into a representation space compatible with LLMs.\nAt the heart of this pipeline, the vision encoder plays a central role, as its ability to extract semantically\nrich visual features directly impacts the generation and reasoning capabilities of the VLM.\nRecent studies have shown that incorporating multiple vision experts improves performance by a\nlarge margin [11, 12, 13, 14, 15]. Nevertheless, these gains in effectiveness often come at the cost of\nefficiency [16, 17, 18, 19, 20]: multi-expert setups require computing visual tokens from all vision\nexperts during both training and inference, making them expensive and less practical for deployment,\nespecially in latency-sensitive or resource-constrained settings [ 21, 22, 23]. As a result, there is\ngrowing interest in approaches that can retain the benefits of multiple vision experts while avoiding\ntheir substantial inference-time costs.\nKnowledge distillation (KD) [24], as a general framework for transferring knowledge from a larger\nmodel (teacher) to a smaller model (student), has been widely used in various domains [25, 26, 27, 28].\nAs a pioneer study of KD in VLMs, MoVE-KD [29] distills knowledge from multiple visual experts\ninto a single vision encoder using a fixed set of Low-Rank Adaptation (LoRA) adapters [ 30] for\nall teachers, enhancing visual understanding while only adding a small set of trainable parameters.\nPreprint. Under review.",
  "2506.18902v2": "arXiv:2506.18902v2  [cs.AI]  24 Jun 2025\njina-embeddings-v4: Universal Embeddings for\nMultimodal Multilingual Retrieval\nMichael G\u00fcnther\u2217, Saba Sturua\u2217, Mohammad Kalim Akram\u2217,\nIsabelle Mohr\u2217, Andrei Ungureanu\u2217, Bo Wang\u2217, Sedigheh Eslami, Scott Martens,\nMaximilian Werk, Nan Wangand Han Xiao\nJina AI GmbH, Prinzessinnenstra\u00dfe 19, 10969, Berlin, Germany\nresearch@jina.ai\nAbstract\nWe introduce jina-embeddings-v4, a 3.8\nbillion parameter multimodal embedding model\nthat unifies text and image representations\nthrough a novel architecture supporting both\nsingle-vector and multi-vector embeddings\nin the late interaction style. The model incor-\nporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance\nacross diverse retrieval scenarios, including\nquery-document retrieval, semantic text sim-\nilarity, and code search. Comprehensive evalu-\nations demonstrate thatjina-embeddings-v4\nachieves state-of-the-art performance on both\nsingle-modal and cross-modal retrieval tasks,\nwith particular strength in processing visually\nrich content such as tables, charts, diagrams,\nand mixed-media formats. To facilitate\nevaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically\ndesigned for visually rich image retrieval.\n1 Introduction\nWe present jina-embeddings-v4, a multimodal\nembedding model capable of processing text and\nimage data to produce semantic embedding vectors\nof varying lengths, optimized for a broad array\nof applications. It incorporates optimized LoRA\nadapters [Hu et al., 2022] for information retrieval\nand semantic text similarity. An adapter is also\nprovided for programming language embeddings,\ntechnical question-answering, and natural language\ncode retrieval. It also brings new functionality to\nprocessing visually rich images (also called visual\ndocuments), i.e., materials mixing texts and images,\ncontaining tables, charts, diagrams, and other kinds\nof common mixed media [Ding et al., 2024]. We\nhave also developed Jina-VDR, a new multilingual,\nmulti-domain benchmark suite for a broad range\nof visual retrieval tasks, to evaluate the capabilities\nof jina-embeddings-v4.\n*Equal contribution.\nWe discuss the challenges of developing a\nmultimodal, multi-functional, state-of-the-art\nembedding model capable of handling texts in a\nvariety of languages, including computer coding\nlanguages, images, and \u201cvisually rich\u201d data. The\nresulting model, jina-embeddings-v4, projects\ninputs from all modalities into a unified semantic\nspace, minimizing or eliminating the \u201cmodality\ngap\u201d that has troubled similar projects [Liang et al.,\n2022a]. In addition, we introduce Jina-VDR, an\nadvanced benchmark for images like screenshots\nand scans of visually complex documents.\nThe major contributions of this work are as\nfollows:\n\u2022 We introduce a unified multi-task learning\nparadigm that jointly optimizes embedding\nmodels to represent texts and images as single-\nand multi-vector embeddings.\n\u2022 Building on work done for\njina-embeddings-v3, we train LoRA\nextensions to enhance support for specific\ndomains and task types, achieving results\ncomparable to specialized models.\n\u2022 We have made particularly strong progress in\nhandling visually rich images, especially for\ntasks outside of the existing ViDoRe bench-\nmark [Faysse et al., 2025], which is limited to\nquestion-answering. jina-embeddings-v4\noutperforms other multimodal models by a sig-\nnificant margin on this type of material and sup-\nports a much more diverse set of use scenarios.\n\u2022 We construct a multilingual, multi-domain\nbenchmark for screenshot retrieval. In contrast\nto other retrieval benchmarks (i.e., [Faysse\net al., 2025, Xiao et al., 2025]) that focus on\nquestion answering and OCR-related tasks,\nwe expand the scope of visual document\nbenchmarking to multilingual retrieval,\nmore query types, and a much more diverse"
}