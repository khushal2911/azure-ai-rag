{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Evaluation Dataset\n",
    "\n",
    "Source: https://docs.ragas.io/en/latest/getstarted/rag_testset_generation/#choose-your-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "%pip install ragas\n",
    "\n",
    "%pip install unstructured\n",
    "\n",
    "%pip install unstructured[pdf]\n",
    "\n",
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_openai_embeddings_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "azure_openai_api_version = \"2024-12-01-preview\"\n",
    "azure_search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_search_service_admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "azure_search_service_index_name = \"az-search-index-001\"\n",
    "azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents\n",
    "\n",
    "Note: I had an error loading to files first time so I had to run this in the github codespaces terminal:\n",
    "- sudo apt-get update\n",
    "- sudo apt-get install -y libgl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"Data/nasabooks/\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(AzureChatOpenAI(\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    validate_base_url=False,\n",
    "    api_key=azure_openai_key\n",
    "))\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(AzureOpenAIEmbeddings(\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=azure_openai_embeddings_deployment,\n",
    "    model=azure_openai_embeddings_deployment,\n",
    "    api_key=azure_openai_key\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c033822a5e594b768819b06eeeb013c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbcbf631f7f4f5da7cfba8db87d1af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a86b70916d24a2c92ab52881e6e925a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35785285f75b471b898523990d659b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff80e495c554abea9d942da519d7ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa256c35ee9b4126b802f65ee7129e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374fe232e17c4d77bbc028cadc512714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save just the query and ground truth to a JSONL file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happen off Vancouver Island in August 2016?</td>\n",
       "      <td>[r e t a W\\n\\n58\\n\\nTeeming Life in the Strait...</td>\n",
       "      <td>In August 2016, the waters off of British Colu...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you explain the significance of the South ...</td>\n",
       "      <td>[r e t a W\\n\\n48\\n\\nCoral Cocos Indian Ocean\\n...</td>\n",
       "      <td>The South Keeling Islands, part of the Cocos I...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut significant event in oceanography was obse...</td>\n",
       "      <td>[r e t a W\\n\\n56\\n\\nA Lava Lamp Look at the At...</td>\n",
       "      <td>In April 2013, infrared data collected by the ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What happens to the water in James Bay when ri...</td>\n",
       "      <td>[r e t a W\\n\\n44\\n\\nTea-Colored Rupert Bay Can...</td>\n",
       "      <td>When rivers flow into James Bay, the collision...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do coastal weather patterns in China relat...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nTracing the Coast China\\n\\nThe oce...</td>\n",
       "      <td>Coastal weather patterns in China demonstrate ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0   What happen off Vancouver Island in August 2016?   \n",
       "1  Can you explain the significance of the South ...   \n",
       "2  Wut significant event in oceanography was obse...   \n",
       "3  What happens to the water in James Bay when ri...   \n",
       "4  How do coastal weather patterns in China relat...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [r e t a W\\n\\n58\\n\\nTeeming Life in the Strait...   \n",
       "1  [r e t a W\\n\\n48\\n\\nCoral Cocos Indian Ocean\\n...   \n",
       "2  [r e t a W\\n\\n56\\n\\nA Lava Lamp Look at the At...   \n",
       "3  [r e t a W\\n\\n44\\n\\nTea-Colored Rupert Bay Can...   \n",
       "4  [<1-hop>\\n\\nTracing the Coast China\\n\\nThe oce...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  In August 2016, the waters off of British Colu...   \n",
       "1  The South Keeling Islands, part of the Cocos I...   \n",
       "2  In April 2013, infrared data collected by the ...   \n",
       "3  When rivers flow into James Bay, the collision...   \n",
       "4  Coastal weather patterns in China demonstrate ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text_list):\n",
    "    cleaned_text_list = []\n",
    "    for text in text_list:\n",
    "        # Remove the UUID (assuming it's always at the start and followed by two newlines)\n",
    "        cleaned_text = text.split('\\n\\n', 1)[-1]\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "    return cleaned_text_list\n",
    "\n",
    "# Apply the function to the 'reference_contexts' column to remove UUID at the start\n",
    "df['reference_contexts'] = df['reference_contexts'].apply(clean_text)\n",
    "\n",
    "# Save to CSV file\n",
    "df.to_csv('Data/output/nasabooks-ragas-testset.csv', index=False)\n",
    "\n",
    "# Create a new DataFrame for EvalCollection\n",
    "eval_collection = pd.DataFrame(columns=['query', 'response', 'context', 'ground_truth'])\n",
    "\n",
    "# Populate the new DataFrame\n",
    "eval_collection['query'] = df['user_input']\n",
    "eval_collection['ground_truth'] = df['reference']\n",
    "eval_collection['response'] = ''\n",
    "eval_collection['context'] = ''\n",
    "\n",
    "# Save the DataFrame as a JSONL file\n",
    "eval_collection.to_json('Data/output/nasabooks-groundtruth.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Response and Context from the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalCollection has been saved to nasabooks-evalset.jsonl\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    # to get version: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key)\n",
    "\n",
    "# Provide instructions to the model\n",
    "SYSTEM_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided below.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources below. Cite your source when you answer the question\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "# Iterate over each row in eval_collection\n",
    "for index, row in eval_collection.iterrows():\n",
    "    # User Query\n",
    "    query = row['query']  \n",
    "\n",
    "    # Convert query into vector form\n",
    "    vector_query = VectorizableTextQuery(text=query, \n",
    "                                        k_nearest_neighbors=50, \n",
    "                                        fields=\"text_vector\",\n",
    "                                        weight=1)\n",
    "\n",
    "    results = search_client.search(\n",
    "        query_type=\"semantic\", \n",
    "        semantic_configuration_name='my-semantic-config',\n",
    "        search_text=query,\n",
    "        vector_queries= [vector_query],\n",
    "        select=[\"title\",\"chunk\"],\n",
    "        top=3,\n",
    "    )\n",
    "\n",
    "    # Use a unique separator to make the sources distinct. \n",
    "    # We chose repeated equal signs (=) followed by a newline because it's unlikely the source documents contain this sequence.\n",
    "    sources_formatted = \"=================\\n\".join([f'TITLE: {document[\"title\"]}, CONTENT: {document[\"chunk\"]}' for document in results])\n",
    "\n",
    "    # Update the context in the DataFrame\n",
    "    eval_collection.at[index, 'context'] = sources_formatted\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": SYSTEM_PROMPT.format(query=query, sources=sources_formatted)\n",
    "            }\n",
    "        ],\n",
    "        model=azure_openai_deployment\n",
    "    )\n",
    "\n",
    "    # Update the response in the DataFrame\n",
    "    eval_collection.at[index, 'response'] = response.choices[0].message.content\n",
    "\n",
    "# Save the updated DataFrame as a JSONL file\n",
    "eval_collection.to_json('Data/output/nasabooks-evalset.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Print success message\n",
    "print(\"EvalCollection has been saved to nasabooks-evalset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happen off Vancouver Island in August 2016?</td>\n",
       "      <td>In August 2016, the waters off Vancouver Islan...</td>\n",
       "      <td>TITLE: page-65.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...</td>\n",
       "      <td>In August 2016, the waters off of British Colu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you explain the significance of the South ...</td>\n",
       "      <td>The South Keeling Islands are significant in r...</td>\n",
       "      <td>TITLE: page-55.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...</td>\n",
       "      <td>The South Keeling Islands, part of the Cocos I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut significant event in oceanography was obse...</td>\n",
       "      <td>In April 2013, a significant event in oceanogr...</td>\n",
       "      <td>TITLE: page-63.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...</td>\n",
       "      <td>In April 2013, infrared data collected by the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What happens to the water in James Bay when ri...</td>\n",
       "      <td>When rivers flow into James Bay, particularly ...</td>\n",
       "      <td>TITLE: page-51.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...</td>\n",
       "      <td>When rivers flow into James Bay, the collision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do coastal weather patterns in China relat...</td>\n",
       "      <td>- Coastal weather patterns in China feature on...</td>\n",
       "      <td>TITLE: page-11.pdf, CONTENT: A\\nT\\n\\nM\\nO\\n\\nS...</td>\n",
       "      <td>Coastal weather patterns in China demonstrate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0   What happen off Vancouver Island in August 2016?   \n",
       "1  Can you explain the significance of the South ...   \n",
       "2  Wut significant event in oceanography was obse...   \n",
       "3  What happens to the water in James Bay when ri...   \n",
       "4  How do coastal weather patterns in China relat...   \n",
       "\n",
       "                                            response  \\\n",
       "0  In August 2016, the waters off Vancouver Islan...   \n",
       "1  The South Keeling Islands are significant in r...   \n",
       "2  In April 2013, a significant event in oceanogr...   \n",
       "3  When rivers flow into James Bay, particularly ...   \n",
       "4  - Coastal weather patterns in China feature on...   \n",
       "\n",
       "                                             context  \\\n",
       "0  TITLE: page-65.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...   \n",
       "1  TITLE: page-55.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...   \n",
       "2  TITLE: page-63.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...   \n",
       "3  TITLE: page-51.pdf, CONTENT: W\\na\\n\\nt\\ne\\n\\nr...   \n",
       "4  TITLE: page-11.pdf, CONTENT: A\\nT\\n\\nM\\nO\\n\\nS...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  In August 2016, the waters off of British Colu...  \n",
       "1  The South Keeling Islands, part of the Cocos I...  \n",
       "2  In April 2013, infrared data collected by the ...  \n",
       "3  When rivers flow into James Bay, the collision...  \n",
       "4  Coastal weather patterns in China demonstrate ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_collection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
